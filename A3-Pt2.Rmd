---
title: "TITLE OF YOUR PROJECT"
author: "Group 8"
date: "November 12th, 2024"
output: pdf_document
---

List your group members, including their student numbers, here:

<<<<<<< HEAD

- Aditya Chauhan (169027493)
- John Darnielle (#########)
- Craig Finn (#########)
- Joel Plaskett (########)
- Ezra Furman (#########)
>>>>>>> 5f7fee6584554860c6964f24cabaeb60755a72fe

You **must** be in a group in MyLS in order to see the DropBox used for submission. Even if you're alone, you must join a group by yourself.

You **must** be in a group with people from the same section as you. MyLS does not allow for groups including students from both Data100A and Data100B.

```{r initial_setup, include=FALSE}
# echo = FALSE will set the Rmd to *not* show the R code. Don't change this.
# You may change the default figure width and figure height as you please.
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.width = 6)
# Put any libraries that you need to load here.
# DO NOT PUT "install.packages()" IN AN RMD FILE!!!
library(tidyverse)
library(arrow)
library(maps)
library(tinytex)
```

# Instructions

You are encouraged to remove this instruction section prior to submission.

It is recommended that you follow the structure of this template. The text is all placeholder - you are free to change any/all wording as you please, but it is very helpful for the grading process if you keep the same structure. Anything in \<<double angle brackets>\> definitely needs to be changed, but you are free to change any/all sentences!

Note that all of the code is *hidden* by default. This file will be graded based on the insights, not the code.

You will only submit the PDF version of this document. To knit to PDF, you'll need to run `install.packages("tinytex")` in the console, followed by `tinytex::install_tinytex()` (DO NOT PUT THESE COMMANDS IN AN RMD FILE!!!). If you encounter errors in "Knit to PDF", you can "knit to html" and then print the html file to PDF using your operating system's PDF view (e.g. Adobe Acrobat). Only standalone PDF files will be accepted by MyLS.

# Abstract

General context, very brief data descriptions, techniques used, and general conclusions, all contained within a single, concise paragraph.

# Introduction

Climate change is something that has been studied. Here's some relevant information about the context of our study.

If needed, this paragraph is more information about the context.

In this report, we are going to explore some aspects climate change and the impact and/or perceptions of it by using exploratory techniques. We'll explore \<<general description of data>\> using \<<general description of techniques>\>.

By the end of this report, we will have shown ...

# Data Description

## \<\<Data Set 1\>\>

```{r load_data1}
#ice_extent
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1
cyclones_data <- read_parquet("cyclones_data.parquet")

# Convert latitude(-90/+90) and longitude(-180/+180) to be within the correct range
cyclones_data <- cyclones_data |>
    mutate(
        lat = ((lat + 90) %% 180) - 90,  
        lon = ((lon + 180) %% 360) - 180 
    )

cyclones_data <- cyclones_data |>
    mutate(MonthName = factor(month.abb[Month], levels = month.abb)) |>
    relocate(MonthName, .after = Month)

cyclones_data

# Note that the code in this document will not be shown
# when you click "knit", so the placement of this code
# chunk is purely for your benefit: You can see what happened
# with your data, which makes it easier to describe below!
```

The data come from \<<place>\> and describe \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>.

## \<\<ice_extent_yearly\>\>

```{r load_data2}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1
ice_extent_yearly <- read_parquet("ice_extent_yearly.parquet")
cyclones_data <- read_parquet("cyclones_data.parquet")
happiness_data <- read_parquet("happiness.parquet")

cyclone_yearly <- cyclones_data |>
  group_by(ObservYear) |> 
  summarize(avg_max_wind = mean(max_wind, na.rm = TRUE),
            avg_min_pressure = mean(min_pressure, na.rm = TRUE),
            count = n())

colnames(cyclone_yearly)[1] <- "year" 
ice_arctic <- ice_extent_yearly |>
  filter(region == "Arctic", name == "min") |>
  select(year, arctic_min_extent = value)

ice_antarctic <- ice_extent_yearly |>
  filter(region == "Antarctic", name == "min") |>
  select(year, antarctic_min_extent = value)

happiness_yearly <- happiness_data |>
  group_by(year) |>
  summarize(avg_perceptions_of_corruption = mean(perceptions_of_corruption, na.rm = TRUE))

combined_data <- cyclone_yearly |>
  inner_join(ice_arctic, by = "year") |>
  inner_join(ice_antarctic, by = "year") |>
  inner_join(happiness_yearly, by = "year")


p3 <- ggplot(combined_data, aes(x = avg_perceptions_of_corruption, y = arctic_min_extent)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Correlation between Perceptions of Corruption and Arctic Ice Extent",
       x = "Average Perceptions of Corruption",
       y = "Arctic Minimum Ice Extent (million sq km)") +
  theme_minimal()

p4 <- ggplot(combined_data, aes(x = avg_perceptions_of_corruption, y = antarctic_min_extent)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Correlation between Perceptions of Corruption and Antarctic Ice Extent",
       x = "Average Perceptions of Corruption",
       y = "Antarctic Minimum Ice Extent (million sq km)") +
  theme_minimal()

print(p1)
print(p2)
print(p3)
print(p4)
colnames(happiness_data)
# Reminder: do NOT print your data to the screen unless it's
# completely necessary
```
The data come from \<<place>\> and detail \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>

## \<\<Data Set 3\>\>

```{r load_data3}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1
climate_awareness <- read_parquet("climate_awareness.parquet")

climate_awareness_mapped <- climate_awareness |>
    mutate(country = case_when(
        country == "United States" ~ "USA",
        country == "United Kingdom" ~ "UK",
        country == "Congo (Brazzaville)" ~ "Republic of Congo",
        country == "Congo (Kinshasa)" ~ "Democratic Republic of the Congo",
        country == "Hong Kong S.A.R. of China" ~ "Hong Kong",
        country == "Türkiye" ~ "Turkey",
        country == "Lao.People's.Democratic.Republic" ~ "Laos",
        country == "United Arab Emirates" ~ "UAE",
        TRUE ~ country
    )) |>
    right_join(world_map, by = c("country" = "region"))
climate_awareness_mapped
```

The data come from \<<place>\> and detail \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>

## \<\<Data Set 4\>\>

```{r load_data4}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1
covid_2020 <- read_parquet("covid_2020.parquet")
covid_2020
```

## \<\<Data Set 5\>\>

```{r load_data5}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1
happiness <- read_parquet("happiness.parquet")
happiness_mapped <- happiness |>
    mutate(country = case_when(
        country == "United States" ~ "USA",
        country == "United Kingdom" ~ "UK",
        country == "Congo (Brazzaville)" ~ "Republic of Congo",
        country == "Congo (Kinshasa)" ~ "Democratic Republic of the Congo",
        country == "Hong Kong S.A.R. of China" ~ "Hong Kong",
        country == "Türkiye" ~ "Turkey",
        country == "Lao.People's.Democratic.Republic" ~ "Laos",
        country == "United Arab Emirates" ~ "UAE",
        TRUE ~ country
    )) |>
    right_join(world_map, by = c("country" = "region"))
happiness_mapped
```

## Combining the Data

Explain how any combinations of data were performed. Explain what kind of join was needed, whether columns had to be modified (for example, matching "country" names.)

# Exploratory Data Analysis

To achieve our goals, we explored the data by...

We explored many aspects of the data, but will demonstrate three. These are \<\<insight 1\>\>, \<\<insight 2\>\>, and \<<insight3>\>

The first aspect that we found interesting is shown in \@ref(fig:insight1). The insight should be specific to the data shown, not a general statement beyond the data (leave that for the conclusion).

```{r insight1, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This is an example of how you can control figures and captions in
# an R chunk. Note that you can reference figures using:
# \@ref(fig:insight1), where "insight1" is the label of this code
# chunk (the first bit of text after the "r" in "```{r label, options...}")

# below are possible correlations 
world_map <- map_data("world")

ggplot() +
    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") +
    # hurricanes +-5 of equator are extremely rare
    geom_rect(aes(xmin = -180, xmax = 180, ymin = -5, ymax = 5), fill = "red", alpha = 0.2) + 
    geom_hex(data = cyclones_data, aes(x = lon, y = lat, fill = category), bins = 50, alpha = 0.7) +
    scale_fill_viridis_d(name = "Storm Category") +
    labs(
        title = "North Atlantic and Northeast Pacific Cyclones by Category",
        x = "Longitude",
        y = "Latitude"
    ) +
    scale_x_continuous(breaks = c(-180, -90, 0, 90, 180)) +
    scale_y_continuous(breaks = c(-90, -45, 0, 45, 90)) +
    theme_minimal() 

ggplot() +
    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") +
    geom_rect(aes(xmin = -180, xmax = 180, ymin = -5, ymax = 5), fill = "red", alpha = 0.2) +
    geom_hex(data = cyclones_data, aes(x = lon, y = lat, fill = Basin, alpha = ..count..), bins = 50) +
    scale_fill_viridis_d(name = "Storm Category") +
    scale_alpha(range = c(0.35, 1.5)) +
    labs(
        title = "North Atlantic and Northeast Pacific Cyclones by Basin",
        x = "Longitude",
        y = "Latitude"
    ) +
    scale_x_continuous(breaks = c(-180, -90, 0, 90, 180)) +
    scale_y_continuous(breaks = c(-90, -45, 0, 45, 90)) +
    theme_minimal() 

ggplot(cyclones_data) +
    aes(x = ObservYear, y = MonthName, fill = ..count..) +
    geom_bin2d() +
    scale_fill_distiller(direction = 1, palette = "YlOrRd") +
    labs(
        title = "Storm Frequency by Month and Year",
        x = "Observation Year",
        y = "Month",
        fill = "Frequency"
    ) +
    theme_minimal()


ggplot(cyclones_data) +
    aes(x = ObservYear, fill = category) +
    geom_bar(position = "fill", width = 1) +
    labs(
        title = "Proportion of Storm Categories Over Time",
        x = "Observation Year",
        y = "Proportion",
        fill = "Storm Category"
    ) +
    theme_minimal()










happiness_colors <- c("1" = "#000000", "3" = "#aa0000", "4" = "#f46d43",
                      "5" = "#fdae61", "6" = "#fee08b", "7" = "#66bd63",
                      "8" = "#1a9850")

ggplot(happiness_mapped) +
    aes(x = long, y = lat, group = group, fill = factor(round(life_ladder))) +
    geom_polygon(color = "white") +
    scale_fill_manual(values = happiness_colors, name = "Happiness Score") +
    theme_void() +
    labs(title = "World Happiness Scores by Country")




```

This insight is supported by the summary statistics in table \@ref(tab:summary_stats)

```{r summary_stats}
# Calculate the relevant summary statistics here.
# Note that the "kable" function in the "knitr" package
# is convenient for making nice tables. Other packages can
# do much fancier things with tables, but keep in mind that
# the insights should be the star, not the formatting.
```

The next insight that we found is shown in \@ref(fig:insight2).

```{r insight2, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This figure will have a height of 4 and a width of 6.
# Feel free to change this, and to apply different sizes
# to the other figures you create.
```

Finally, \@ref(fig:insight3) shows ...

```{r insight3, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
```

# Conclusion and Future Work

Overall, we found \<<general ideas>\>.

A second paragraph about our findings.

The next steps in this analysis are...

The limitations of this analysis are as follows. (Do not simply list potential issues with sampling, but relate them to your analysis and how they affect your conclusions. An honest and complete acknowledgement of the limitations makes the analysis more trustworthy.)

# References

I am not strict about MLA or APA style or anything like that. For this report, I would much rather have your citations be easy to match to your insights.

The easiest way is to use Rmd's [footnote](https://bookdown.org/yihui/rmarkdown/markdown-syntax.html#inline-formatting) syntax. This will put a number beside the word where the footnote appears, and the full text of the footnote at the bottom of the page (pdf) or end of the document (html). The syntax is:[^1], where I suggest that you put in something like this[^2] to make references for this assignment.

[^1]: See the source view to see this footnote

[^2]: The relevance to the insight is ... . From \<<name of source and name of article>\>, published on \<<date>\>, url: \<<link to page>\>

Alternatively, you could make a list of citations with their main arguments and why they're relevent to your insights, methods, etc.

The link above also references "bibtex" files. These are also extremely convenient, but have a steep learning curve and they make it difficult to tie them to an insight. If you use bibtext, then make sure that you provide a sentence to describe the source and it's relevance when you cite it - don't just add citations to the end of a sentence (this is common practice in academia, but I want to know that your citations are directly relevant for this assignmnet).